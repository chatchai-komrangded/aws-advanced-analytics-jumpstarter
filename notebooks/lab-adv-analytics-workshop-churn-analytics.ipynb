{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Predictive Analytics using Amazon SageMaker\n",
    "\n",
    "---\n",
    "## Background\n",
    "\n",
    "The purpose of this lab is to demonstrate the basics of building an advanced analytics solution using Amazon SageMaker. In this notebook we will create a customer churn analytics solution by training an XGBoost churn model, and batching churn prediction scores into a data warehouse. \n",
    "\n",
    "This notebook extends one of the example tutorial notebooks: [Customer Churn Prediction with XGBoost](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn_neo.ipynb). The extended learning objectives are highlighted in bold below.\n",
    "\n",
    "#### Learning Objectives \n",
    "\n",
    " - **Learn how to query ground truth data from our data warehouse into a pandas dataframe for exploration and feature engineering.**\n",
    " - Train an XGBoost model to perform churn prediction.\n",
    " - **Learn how to run a Batch Transform job to calculate churn scores in batch.**\n",
    " - Optimize your model using SageMaker Neo.\n",
    " - **Run a Glue job programatically to demonstrate data processing and feature engineering at scale using SparkML.88\n",
    " - **Create a production scale inference pipeline that consists of a SparkML feature engineering pipeline that feeds into an XGBoost churn classification model.**\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "The lab guide [here](https://github.com/dylan-tong-aws/aws-advanced-analytics-jumpstarter/blob/master/lab-guides/SageMaker%20Lab-Churn%20Predictive%20Analytics.pdf) takes you through the prerequite steps required by this notebook. \n",
    "\n",
    "In summary:\n",
    " - You've built the lab environment using this CloudFormation [template](https://github.com/dylan-tong-aws/aws-advanced-analytics-jumpstarter/blob/master/cf-templates/adv-analytics-lab.yaml). This template launches a Redshift cluster in your default VPC.\n",
    " - You've taken note of the Redshift cluster credentials, endpoint, and the workshop IAM role ARN that was created by the template.\n",
    " - This notebook should be running in your default VPC. \n",
    " - The security groups on your Redshift cluster and your notebook instance must be able to communicate with each other using TCP on port 5439.\n",
    " \n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook should run on any Amazon SageMaker notebook instance_\n",
    "\n",
    "Let's first install the Redshift drivers on this notebook instance, so that we can query Redshift data directly from our notebook. You could have installed the drivers at launch time using SageMaker [lifecycle configurations](https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y -c anaconda psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. Replace <<'REPLACE WITH YOUR BUCKET NAME'>> with the name of your bucket.\n",
    "\n",
    "- The IAM role arn used to give training and hosting access to your data. By default, we'll use the IAM permissions that have been allocated to your notebook instance. The role should have the permissions to access your S3 bucket, and full execution permissions on Amazon SageMaker. In practice, you could minimize the scope of requried permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import psycopg2\n",
    "import sqlalchemy as sa\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "##bucket = 'dtong-jumpstarter-workshop'\n",
    "bucket = '<<REPLACE WITH YOUR BUCKET NAME>>'\n",
    "prefix = 'churn-analytics-lab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "Mobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes–after all, predicting the future is tricky business! But I’ll also show how to deal with prediction errors.\n",
    "\n",
    "The dataset we use is publicly available and was mentioned in the book [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets.  Let's download and read that dataset in now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://dataminingconsultant.com/DKD2e_data_sets.zip\n",
    "!unzip -o DKD2e_data_sets.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often desireable to leverage data sources from your databases to train machine learning models. The data is in tabular format, and often contains ground truth.\n",
    "\n",
    "In other situations, you might want to load data into a database for the purpose of being to run queries at scale to facilitate exploration and experimentation.\n",
    "\n",
    "In the following steps, we're going to load our data set into our database to demonstrate how you can query data from your notebook into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASOURCE_PREFIX = '{}/datasets/raw'.format(prefix)\n",
    "DATASOURCE_S3URI = 's3://{}/{}/{}'.format(bucket,DATASOURCE_PREFIX,'churn.csv')\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(DATASOURCE_PREFIX,'churn.csv')).upload_file('Data sets/churn.txt')\n",
    "\n",
    "print(\"Data set transferred to your S3 bucket at: {}\".format(DATASOURCE_S3URI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the connection and credentials required to connect to your Redshift instance. You'll need to modify the cell below with the appropriate **host endpoint** and **password** to your database. If you followed the lab guide instructions, you should have this information.\n",
    "\n",
    "In practice, security standards might prohibit you from providing credentials in clear text. As a best practice in production, you should utilize a service like [AWS Secrets Manager](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html) to manage your database credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = {\n",
    "    \"host_name\": \"<<YOUR CLUSTER ENDPOINT>>\",\n",
    "    \"port_num\": \"5439\",\n",
    "    \"db_name\": \"workshop\",\n",
    "    \"username\": \"admin\",\n",
    "    \"password\": \"<<YOUR PASSWORD>>\"\n",
    "}\n",
    "print(creds)\n",
    "\n",
    "def get_conn(creds): \n",
    "    conn = psycopg2.connect(dbname=creds['db_name'], \n",
    "                            user=creds['username'], \n",
    "                            password=creds['password'],\n",
    "                            port=creds['port_num'],\n",
    "                            host=creds['host_name'])\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set **REDSHIFT_IAM_ROLE** to the value of the IAM Role ARN (eg. arn:aws:iam::803235869972:role/workshop-rWorkshopRole-TPJWUEY4PMP6) that was created for you by the CloudFormation template that you executed during the lab setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDSHIFT_IAM_ROLE = '<<SET YOUR IAM ROLE>>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the data into our database cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'CREATE TABLE call_stats( \\\n",
    "  state VARCHAR NOT NULL, \\\n",
    "  acctlen INT NOT NULL, \\\n",
    "  areacode INT2 NOT NULL, \\\n",
    "  phone VARCHAR NOT NULL, \\\n",
    "  intlplan VARCHAR NOT NULL, \\\n",
    "  vmailplan VARCHAR NOT NULL, \\\n",
    "  vmailmsg INT NOT NULL, \\\n",
    "  daymins REAL NOT NULL, \\\n",
    "  daycalls INT NOT NULL, \\\n",
    "  daycharge REAL NOT NULL, \\\n",
    "  evemins REAL NOT NULL, \\\n",
    "  evecalls INT NOT NULL, \\\n",
    "  evecharge REAL NOT NULL, \\\n",
    "  nightmins REAL NOT NULL, \\\n",
    "  nightcalls INT NOT NULL, \\\n",
    "  nightcharge REAL NOT NULL, \\\n",
    "  intlmins REAL NOT NULL, \\\n",
    "  intlcalls INT NOT NULL, \\\n",
    "  intlcharge REAL NOT NULL, \\\n",
    "  custservcalls INT NOT NULL, \\\n",
    "  churn VARCHAR);'\n",
    "\n",
    "def create_table() :\n",
    "    try :\n",
    "        conn = None\n",
    "        with get_conn(creds) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(cmd)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "                \n",
    "create_table()\n",
    "\n",
    "cmd = \"COPY call_stats from \\\n",
    "'{}' IAM_ROLE '{}' DELIMITER ',' IGNOREHEADER as 1\".format(DATASOURCE_S3URI,REDSHIFT_IAM_ROLE);\n",
    "\n",
    "def load_data() :\n",
    "    try :\n",
    "        conn = None\n",
    "        with get_conn(creds) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(cmd)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "start = time.time()         \n",
    "load_data()\n",
    "end = time.time()\n",
    "\n",
    "print(\"Data was loaded in {} seconds.\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Explore\n",
    "\n",
    "Now we can run queries against your database. In the cell below, we retrieve the entire table. \n",
    "  *query = 'select * from public.call_stats;'*\n",
    "\n",
    "However, in practice, the data table will often contain more data than what is practical to operate on within a notebook instance, or relevant attributes are spread across multiple tables. Being able to run SQL queries and loading the data into a pandas dataframe will be helpful during the initial stages of development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query for testing\n",
    "query = 'select * from public.call_stats;'\n",
    "\n",
    "colnames = ['State','Account Length','Area Code','Phone','Intl Plan', 'VMail Plan', 'VMail Message','Day Mins',\n",
    "            'Day Calls', 'Day Charge', 'Eve Mins', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Night Calls','Night Charge',\n",
    "            'Intl Mins','Intl Calls','Intl Charge','CustServ Calls', 'Churn?']\n",
    "\n",
    "def get_df(creds, query):\n",
    "    with get_conn(creds) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            result_set = cur.fetchall()\n",
    "          #  colnames = [desc.name for desc in cur.description]\n",
    "            df = pd.DataFrame.from_records(result_set, columns=colnames)\n",
    "    return df\n",
    "\n",
    "churn = get_df(creds, query)\n",
    "\n",
    "display(churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By modern standards, it’s a relatively small dataset, with only 3,333 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are:\n",
    "\n",
    "- `State`: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ\n",
    "- `Account Length`: the number of days that this account has been active\n",
    "- `Area Code`: the three-digit area code of the corresponding customer’s phone number\n",
    "- `Phone`: the remaining seven-digit phone number\n",
    "- `Int’l Plan`: whether the customer has an international calling plan: yes/no\n",
    "- `VMail Plan`: whether the customer has a voice mail feature: yes/no\n",
    "- `VMail Message`: presumably the average number of voice mail messages per month\n",
    "- `Day Mins`: the total number of calling minutes used during the day\n",
    "- `Day Calls`: the total number of calls placed during the day\n",
    "- `Day Charge`: the billed cost of daytime calls\n",
    "- `Eve Mins, Eve Calls, Eve Charge`: the billed cost for calls placed during the evening\n",
    "- `Night Mins`, `Night Calls`, `Night Charge`: the billed cost for calls placed during nighttime\n",
    "- `Intl Mins`, `Intl Calls`, `Intl Charge`: the billed cost for international calls\n",
    "- `CustServ Calls`: the number of calls placed to Customer Service\n",
    "- `Churn?`: whether the customer left the service: true/false\n",
    "\n",
    "The last attribute, `Churn?`, is known as the target attribute–the attribute that we want the ML model to predict.  Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification.\n",
    "\n",
    "Let's begin exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=churn[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "# Histograms for each numeric features\n",
    "display(churn.describe())\n",
    "%matplotlib inline\n",
    "hist = churn.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see immediately that:\n",
    "- `State` appears to be quite evenly distributed\n",
    "- `Phone` takes on too many unique values to be of any practical use.  It's possible parsing out the prefix could have some value, but without more context on how these are allocated, we should avoid using it.\n",
    "- Only 14% of customers churned, so there is some class imabalance, but nothing extreme.\n",
    "- Most of the numeric features are surprisingly nicely distributed, with many showing bell-like gaussianity.  `VMail Message` being a notable exception (and `Area Code` showing up as a feature we should convert to non-numeric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop('Phone', axis=1)\n",
    "churn['Area Code'] = churn['Area Code'].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at the relationship between each of the features and our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    if column != 'Churn?':\n",
    "        display(pd.crosstab(index=churn[column], columns=churn['Churn?'], normalize='columns'))\n",
    "\n",
    "for column in churn.select_dtypes(exclude=['object']).columns:\n",
    "    print(column)\n",
    "    hist = churn[[column, 'Churn?']].hist(by='Churn?', bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly we see that churners appear:\n",
    "- Fairly evenly distributed geographically\n",
    "- More likely to have an international plan\n",
    "- Less likely to have a voicemail plan\n",
    "- To exhibit some bimodality in daily minutes (either higher or lower than the average for non-churners)\n",
    "- To have a larger number of customer service calls (which makes sense as we'd expect customers who experience lots of problems may be more likely to churn)\n",
    "\n",
    "In addition, we see that churners take on very similar distributions for features like `Day Mins` and `Day Charge`.  That's not surprising as we'd expect minutes spent talking to correlate with charges.  Let's dig deeper into the relationships between our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(churn.corr())\n",
    "pd.plotting.scatter_matrix(churn, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see several features that essentially have 100% correlation with one another.  Including these feature pairs in some machine learning algorithms can create catastrophic problems, while in others it will only introduce minor redundancy and bias.  Let's remove one feature from each of the highly correlated pairs: Day Charge from the pair with Day Mins, Night Charge from the pair with Night Mins, Intl Charge from the pair with Intl Mins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop(['Day Charge', 'Eve Charge', 'Night Charge', 'Intl Charge'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up our dataset, let's determine which algorithm to use.  As mentioned above, there appear to be some variables where both high and low (but not intermediate) values are predictive of churn.  In order to accommodate this in an algorithm like linear regression, we'd need to generate polynomial (or bucketed) terms.  Instead, let's attempt to model this problem using gradient boosted trees.  Amazon SageMaker provides an XGBoost container that we can use to train in a managed, distributed setting, and then host as a real-time prediction endpoint.  XGBoost uses gradient boosted trees which naturally account for non-linear relationships between features and the target variable, as well as accommodating complex interactions between features.\n",
    "\n",
    "Amazon SageMaker XGBoost can train on data in either a CSV or LibSVM format.  For this example, we'll stick with CSV.  It should:\n",
    "- Have the predictor variable in the first column\n",
    "- Not have a header row\n",
    "\n",
    "But first, let's convert our categorical features into numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.get_dummies(churn)\n",
    "model_data = pd.concat([model_data['Churn?_True.'], model_data.drop(['Churn?_False.', 'Churn?_True.'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's split the data into training, validation, and test sets.  This will help prevent us from overfitting the model, and allow us to test the models accuracy on data it hasn't already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "display(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload these files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Moving onto training, first we'll need to specify the locations of the XGBoost algorithm containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "xgb_training_container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters.  A few key hyperparameters are:\n",
    "- `max_depth` controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` controls sampling of the training data.  This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` controls the number of boosting rounds.  This is essentially the subsequent models that are trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` controls how aggressively trees are grown.  Larger values lead to more conservative models.\n",
    "\n",
    "More detail on XGBoost's hyperparmeters can be found on their GitHub [page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(xgb_training_container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m5.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile\n",
    "[Amazon SageMaker Neo](https://aws.amazon.com/sagemaker/neo/) optimizes models to run up to twice as fast, with no loss in accuracy. When calling `compile_model()` function, we specify the target instance family (m4) as well as the S3 bucket to which the compiled model would be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = xgb\n",
    "try:\n",
    "    xgb.create_model()._neo_image_account(boto3.Session().region_name)\n",
    "except:\n",
    "    print('Neo is not currently supported in', boto3.Session().region_name)\n",
    "else:\n",
    "    output_path = '/'.join(xgb.output_path.split('/')[:-1])\n",
    "    compiled_model = xgb.compile_model(target_instance_family='ml_m5', \n",
    "                                   input_shape={'data':[1, 69]},\n",
    "                                   role=role,\n",
    "                                   framework='xgboost',\n",
    "                                   framework_version='0.7',\n",
    "                                   output_path=output_path)\n",
    "    compiled_model.name = 'deployed-xgboost-customer-churn'\n",
    "    compiled_model.image = get_image_uri(sess.boto_region_name, 'xgboost-neo', repo_version='latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference\n",
    "\n",
    "Next we're going to evaluate our model by using a Batch Transform to generate churn scores in batch from our `test_data.`\n",
    "\n",
    "First, we upload the test data to S3. SageMaker Batch Transform is designed to run asynchronously and ingest input data from S3. This differs from SageMaker's real-time inference endpoints, which receive input data from synchronous HTTP requests.\n",
    "\n",
    "Batch Transform is often the ideal option for advanced analytics use case for serveral reasons:\n",
    "\n",
    " - Batch Transform is better optimized for throughput in comparison with real-time inference endpoints. Thus, Batch Transform is ideal for processing large volumes of data for analytics.\n",
    " - Offline asynchronous processing is acceptable for most analytics use cases.\n",
    " - Batch Transform is more cost efficient when real-time inference isn't necessary. You only need to pay for resources used during batch processing. There is no need to pay for ongoing resources like a hosted endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = test_data.iloc[:,1:]\n",
    "batch_input.to_csv('test.csv', header=False, index=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "\n",
    "s3uri_batch_input ='s3://{}/{}/test'.format(bucket, prefix)\n",
    "print('Batch Transform input S3 uri: {}'.format(s3uri_batch_input))\n",
    "\n",
    "s3uri_batch_output= 's3://{}/{}/out'.format(bucket, prefix)\n",
    "print('Batch Transform output S3 uri: {}'.format(s3uri_batch_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "BATCH_INSTANCE_TYPE = 'ml.c5.xlarge'\n",
    "\n",
    "transformer = compiled_model.transformer(instance_count=1,\n",
    "                                         strategy='SingleRecord',\n",
    "                                         assemble_with='Line',\n",
    "                                         instance_type= BATCH_INSTANCE_TYPE,\n",
    "                                         accept = 'text/csv',\n",
    "                                         output_path=s3uri_batch_output)\n",
    "    \n",
    "transformer.transform(s3uri_batch_input, split_type= 'Line', content_type= 'text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to compare the performance of a machine learning model, but let's start by simply by comparing actual to predicted values.  In this case, we're simply predicting whether the customer churned (`1`) or not (`0`), which produces a simple confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_churn_scores = pd.read_csv(s3uri_batch_output+'/test.csv.out', names=['scores'])\n",
    "gt_df = pd.DataFrame(test_data['Churn?_True.']).reset_index(drop=True)\n",
    "results_df= pd.concat([gt_df,batched_churn_scores],axis=1,join_axes=[gt_df.index])\n",
    "\n",
    "pd.crosstab(index=results_df['Churn?_True.'], columns=np.round(results_df['scores']), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Scaling Your Solution\n",
    "\n",
    "In the previous section we used pandas for data wrangling. This is convenient for exploration and prototyping. For small to moderate data sets, you could run your scripts in Lambda as a quick and simple pipelining solution.\n",
    "\n",
    "However, for large data sets, you'll want to leverage a data processing framework like Spark. In the following section, we'll demonstrate the use of [AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html) to serve as a data pipeline for both training and inference. AWS Glue is a serverless ETL service that is built on Apache Spark. \n",
    "\n",
    "---\n",
    "\n",
    "### Training Pipelining\n",
    "\n",
    "Run the next cell to view the PySpark script that performs the equivalent transformations that were previously accomplished using Pandas.\n",
    "\n",
    "This script was developed within an AWS Glue [development environment](https://docs.aws.amazon.com/glue/latest/dg/notebooks-with-glue.html). AWS Glue provides managed notebooks for Zeppelin and an integration with SageMaker managed Jupyter notebooks. \n",
    "\n",
    "To keep this lab brief, we fast forward to the stage where we've already developed our PySpark scripts. This notebook demonstrates the execution of a Glue ETL job through the SDK within a Jupyter notebook. In practice, the AWS Glue job could be launched automatically using AWS Glue's [workflow functionality](https://docs.aws.amazon.com/glue/latest/dg/trigger-job.html), or via an external orchestration tool (eg. Apache Airflow, AWS Step Functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ../scripts/churn-analytics-data-pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's make preparation for our AWS Glue ETL Job. We do the following in the cell below:\n",
    " - Upload the PySpark scripts to an S3 location where AWS Glue can pick it up.\n",
    " - Set the location of the input data\n",
    " - Set the location for the job to output the processed data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_location_prefix = '{}/scripts/churn-analytics-data-pipeline.py'.format(prefix)\n",
    "script_location = 's3://{}/{}'.format(bucket,script_location_prefix)\n",
    "\n",
    "start = time.time()\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(script_location_prefix).upload_file('../scripts/churn-analytics-data-pipeline.py')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Scripts were uploaded to {} in {} seconds.\".format(script_location,end-start))\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# Input location of the data, We uploaded our train.csv file to input key previously\n",
    "s3_input_bucket = bucket\n",
    "s3_input_key_prefix = DATASOURCE_PREFIX\n",
    "\n",
    "# Output location of the data. The input data will be split, transformed, and \n",
    "# uploaded to output/train and output/validation\n",
    "s3_output_bucket = bucket\n",
    "s3_output_key_prefix = timestamp_prefix + '/churn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload MLeap dependencies to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our job, we will also have to pass MLeap dependencies to Glue. MLeap is an additional library we are using which does not come bundled with default Spark.\n",
    "\n",
    "MLeap provides a means to export Spark pipelines into a portable format, and execution engine. In the above AWS Glue job, we perform a number of data transformations which we export using MLeap. This allows us to later deploy this transformation pipeline into our SageMaker production environment as part of an inference pipeline.\n",
    "\n",
    "Similar to most of the packages in the Spark ecosystem, MLeap is also implemented as a Scala package with a front-end wrapper written in Python so that it can be used from PySpark. We need to make sure that the MLeap Python library as well as the JAR is available within the Glue job environment. In the following cell, we will download the MLeap Python dependency & JAR from a SageMaker hosted bucket and upload to the S3 bucket we created above in your account. \n",
    "\n",
    "If you are using some other Python libraries like `nltk` in your code, you need to download the wheel file from PyPI and upload to S3 in the same way. At this point, Glue only supports passing pure Python libraries in this way (e.g. you can not pass `Pandas` or `OpenCV`). However you can use `NumPy` & `SciPy` without having to pass these as packages because these are pre-installed in the Glue environment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/python/python.zip\n",
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we upload the libraries to an S3 location that our AWS Glue job can pick up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_dep_location = sess.upload_data(path='python.zip', bucket=bucket, key_prefix='dependencies/python')\n",
    "jar_dep_location = sess.upload_data(path='mleap_spark_assembly.jar', bucket=bucket, key_prefix='dependencies/jar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll have our AWS Glue job upload our MLeap serialized SparkML model at the following location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_bucket = bucket\n",
    "s3_model_key_prefix = s3_output_key_prefix + '/mleap'\n",
    "\n",
    "print('AWS Glue will upload the MLeap model to {}/{}'.format(s3_model_bucket,s3_model_key_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a Glue Job\n",
    "\n",
    "Next we'll be creating a Glue client via Boto so that we can invoke the `create_job` API for Glue. `create_job` will create a job definition which can be executed to run our script.\n",
    "\n",
    "`AllocatedCapacity` parameter controls the hardware resources that Glue will use to execute this job. It is measures in units of `DPU`. For more information on `DPU`, please see [here](https://docs.aws.amazon.com/glue/latest/dg/add-job.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue')\n",
    "job_name = 'churn-analytics-pipeline-' + timestamp_prefix\n",
    "response = glue_client.create_job(\n",
    "    Name=job_name,\n",
    "    Description='PySpark job to featurize the telco churn dataset',\n",
    "    Role=role, # you can pass your existing AWS Glue role here if you have used Glue before\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 1\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': script_location\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python',\n",
    "        '--extra-jars' : jar_dep_location,\n",
    "        '--extra-py-files': python_dep_location\n",
    "    },\n",
    "    AllocatedCapacity=5,\n",
    "    Timeout=60,\n",
    ")\n",
    "glue_job_name = response['Name']\n",
    "print('Create an AWS Glue job definition named {}'.format(glue_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to run our AWS Glue job. Next we call the `start_job_run` API to execute the job. We provide the parameters that we've set in the previous steps to specify the data input, output, script and model locations in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_job_args = {\n",
    "                    '--S3_INPUT_BUCKET': s3_input_bucket,\n",
    "                    '--S3_INPUT_KEY_PREFIX': s3_input_key_prefix,\n",
    "                    '--S3_OUTPUT_BUCKET': s3_output_bucket,\n",
    "                    '--S3_OUTPUT_KEY_PREFIX': s3_output_key_prefix,\n",
    "                    '--S3_MODEL_BUCKET': s3_model_bucket,\n",
    "                    '--S3_MODEL_KEY_PREFIX': s3_model_key_prefix\n",
    "                }\n",
    "    \n",
    "job_run_id = glue_client.start_job_run(JobName=job_name,\n",
    "                                       Arguments = glue_job_args)['JobRunId']\n",
    "print('Running Job ID: {}'.format(job_run_id))\n",
    "print('Arguments provided: {}'.format(glue_job_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Glue job status\n",
    "\n",
    "Now we will check for the job status to see if it has `succeeded`, `failed` or `stopped`. Once the job is succeeded, we have the transformed data into S3 in CSV format which we can use with XGBoost for training. If the job fails, you can go to [AWS Glue console](https://us-west-2.console.aws.amazon.com/glue/home), click on **Jobs** tab on the left, and from the page, click on this particular job and you will be able to find the CloudWatch logs (the link under **Logs**) link for these jobs which can help you to see what exactly went wrong in the job execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run_status = glue_client.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
    "while job_run_status not in ('FAILED', 'SUCCEEDED', 'STOPPED'):\n",
    "    job_run_status = glue_client.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n",
    "    print (job_run_status)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Your Churn Predictor\n",
    "\n",
    "The AWS Glue job processes our raw data, and produces training and validation sets that are suited for training our XGBoost model. AWS Glue is built on Apache Spark, so it's designed to easily scale-out and process large data volumes. We simply need to dial up the number of DPUs to increase resources available to drive up the parallel processing power of our Spark cluster.\n",
    "\n",
    "In this lab, we operate on a small dataset, so there's no value in retraining our XGBoost model. To save on time and resources, we leverage the XGBoost model that we've already trained. However, the AWS Glue job did serve teh purpose of generating our SparkML pipeline model, which we'll use in the following sections to build an inference pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Inference Pipelining\n",
    "\n",
    "Next we will create an inference pipeline that consists of our SparkML model and our XGBoost model. The pipeline is able to take raw data as input, leverage our SparkML pipeline for data processing (feature engineering), and then generate churn scores using our XGBoost model as a Batch Transform (alternatively, a real-time inference endpoint could use used). This type of inference pipeline is practical when pre-processing isn't viable, and you want to either perform near real-time inference, or escapsulate your batch processing pipeline into a single model. \n",
    "\n",
    "---\n",
    "\n",
    "Deploying a model in SageMaker requires two components:\n",
    "\n",
    "* Docker image residing in ECR.\n",
    "* Model artifacts residing in S3.\n",
    "\n",
    "**SparkML**\n",
    "\n",
    "* SageMaker provides a Docker image for SparkML running on MLeap. For more information, please see [SageMaker SparkML Serving](https://github.com/aws/sagemaker-sparkml-serving-container). \n",
    "* The model artifacts for our SparkML model was seralized by MLeap by the AWS Glue job that we ran previously.\n",
    "\n",
    "**XGBoost**\n",
    "\n",
    "* SageMaker provides Docker images for all it's algorithms. We can use the same XGBoost container as we did for training. \n",
    "* The model artifacts for our XGBoost model was created by our SageMaker training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkML Container: Define a Schema\n",
    "\n",
    "The SparkML serving container requires a schema for the `predict` method call. In order to alleviate the pain of not having to pass the schema with every request, `sagemaker-sparkml-serving` allows you to set the schema once through the model definitions.\n",
    "\n",
    "We'll do just that in the following cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"input\": [{\"name\": \"State\",\"type\": \"string\"}, \n",
    "              {\"name\": \"AccountLength\", \"type\": \"int\"}, \n",
    "              {\"name\": \"AreaCode\",\"type\": \"int\"}, \n",
    "              {\"name\": \"Phone\",\"type\": \"string\"}, \n",
    "              {\"name\": \"IntlPlan\",\"type\": \"string\"}, \n",
    "              {\"name\": \"VMailPlan\",\"type\": \"string\"},\n",
    "              {\"name\": \"VMailMessage\",\"type\": \"int\"},\n",
    "              {\"name\": \"DayMins\",\"type\": \"float\"},\n",
    "              {\"name\": \"DayCalls\",\"type\": \"int\"},\n",
    "              {\"name\": \"DayCharge\",\"type\": \"float\"},\n",
    "              {\"name\": \"EveMins\",\"type\": \"float\"},\n",
    "              {\"name\": \"EveCalls\",\"type\": \"int\"},\n",
    "              {\"name\": \"EveCharge\",\"type\": \"float\"},\n",
    "              {\"name\": \"NightMins\",\"type\": \"float\"},\n",
    "              {\"name\": \"NightCalls\",\"type\": \"int\"},\n",
    "              {\"name\": \"NightCharge\",\"type\": \"float\"},\n",
    "              {\"name\": \"IntlMins\",\"type\": \"float\"},\n",
    "              {\"name\": \"IntlCalls\",\"type\": \"int\"},\n",
    "              {\"name\": \"IntlCharge\",\"type\": \"float\"},\n",
    "              {\"name\": \"CustServCalls\",\"type\": \"int\"},],\n",
    "    \"output\": {\"name\": \"features\",\"type\": \"double\",\"struct\": \"vector\"}\n",
    "}\n",
    "\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a `PipelineModel`\n",
    "\n",
    "Next we'll create a SageMaker `PipelineModel` with SparkML and XGBoost. The `PipelineModel` will ensure that the containers and models are deployed behind a single API endpoint, and are operated on in the specified order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'xgboost', repo_version=\"latest\")\n",
    "print (training_image)\n",
    "\n",
    "sparkml_data = 's3://{}/{}/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n",
    "\n",
    "sparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\n",
    "xgb_model = Model(model_data=xgb.model_data, image=xgb_training_container)\n",
    "\n",
    "model_name = 'inference-pipeline-' + timestamp_prefix\n",
    "sm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we register this PipelineModel with SageMaker, so that it can be used for things like a Batch Transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model.sagemaker_session = sess\n",
    "container_def = sm_model.pipeline_container_def(instance_type=BATCH_INSTANCE_TYPE)\n",
    "sess.create_model(model_name, role, container_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Inference Pipeline\n",
    "\n",
    "MLeap can potentially offer significant performance improvements in production. Refer to the following benchmarks, which demonstrate inference times that are magnitudes quicker:\n",
    "https://github.com/combust/mleap/blob/master/mleap-benchmark/README.md\n",
    "\n",
    "In the next step, we're going to run our inference pipeline in batch. This differs from the previous SageMaker Batch Transform job, which operated on data sets that we had pre-process. In this case, our Batch Transform uses our PipelineModel, which operates on our raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................!\n"
     ]
    }
   ],
   "source": [
    "s3uri_batch_output = 's3://{}/{}/pipeline/out'.format(bucket, prefix)\n",
    "s3uri_batch_input= 's3://reinvent2018-sagemaker-pytorch/datasets/uci-telco-churn/raw/churn_nolabels.csv'    \n",
    "\n",
    "transformer = sagemaker.transformer.Transformer(model_name = model_name,\n",
    "                                                instance_count=1,\n",
    "                                                strategy='SingleRecord',\n",
    "                                                assemble_with='Line',\n",
    "                                                instance_type= BATCH_INSTANCE_TYPE,\n",
    "                                                accept = 'text/csv',\n",
    "                                                sagemaker_session=sess,\n",
    "                                                base_transform_job_name='serial-inference-batch',\n",
    "                                                output_path=s3uri_batch_output)\n",
    " \n",
    "transformer.transform(s3uri_batch_input, split_type= 'Line', content_type= 'text/csv')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Batch Transform job transforms your raw data into churn scores, which can then be loaded into your database. With Redshift, you can run a COPY command as demonstrated earlier in the lab. \n",
    "\n",
    "In practice, this pipeline should be automated end-to-end using an orchestration tool like Apache Airflow or AWS Step Functions. The Batch Transform job could be kicked off on a schedule, or based on an S3 event when a daily batch of data lands in the data lake. Once the Batch Transform job completes successfully, the workflow can simply load the churn scores into your datawarehouse, and your BI users will be enabled to run predictive churn analytics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
